# complete-deep-learning
Comprehensive Deep Learning concepts &amp; Architectures implemented using PyTorch.

# content-structure
```
<b>complete-deep-learning</b>
├── datasets
│   ├── images                          # Image datasets (e.g., CIFAR, ImageNet)
│   ├── text                            # Text datasets (e.g., IMDB reviews, Wikipedia)
│   ├── audio                           # Audio datasets (e.g., Speech commands)
│   └── misc                            # Other formats (.csv, .json, .xlsx, etc.)
├── basic-neural-network-architecture
│   ├── neuron (perceptron)
│   ├── neural-net-layers
│   ├── activation-functions
│   ├── ann
│   │   ├── geometric-view
│   │   ├── ann-maths
│   │   ├── ann-regression
│   │   ├── ann-clasification
│   │   ├── multi-layer-ann
│   │   ├── multi-output-ann
│   │   ├── random-forest
│   │   └── model-deptth-breadth
│   ├── meta-parameters
│   └── hyper-parameters
├── <b>neural-network-concepts </b>
│   ├── regularization
│   │   ├── prevent-overfitting-underfitting
│   │   ├── weight-reg
│   │   ├── dropout
│   │   ├── data-augmentation
│   │   ├── nomralization
│   │   │   ├── batch-nomralization
│   │   │   └── layer-nomralization
│   │   └── early-stopping
│   ├── optimization
│   │   ├── loss-cost-functions
│   │   ├── gradient-descent
│   │   ├── adaptive-optimization-algorithms
│   │   ├── learning-schedules
│   │   ├── weight-investigations
│   │   ├── numerical-stability
│   │   ├── meta-parameter optimization
│   │   └── hyper-parameter optimization
│   └── generalization
│   │   ├── cross-validation
│   │   ├── overfitting-underfitting
│   │   └── hyper-parameter-tuning
├── <b>advanced-neural-network-architecture</b>
│   ├── ffn
│   ├── cnn
│   │   ├── convolution
│   │   └── cnn-architecture
│   ├── rnn
│   │   ├── lstm
│   │   ├── gru
│   ├── gan
│   ├── gnn
│   ├── attention-mechanism
│   ├── transformer-models
│   │   └── bert
│   └── auto-encoders
└── <b>model-training</b>
```
