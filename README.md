# complete-deep-learning
Comprehensive Deep Learning concepts &amp; Architectures implemented using PyTorch.

## content-structure
```
complete-deep-learning
├── datasets
│   ├── images                         
│   ├── text                        
│   ├── audio                        
│   └── misc                           
├── basic-neural-network-architecture
│   ├── neuron (perceptron)
│   ├── neural-net-layers
│   ├── activation-functions
│   ├── ann
│   │   ├── geometric-view
│   │   ├── ann-maths
│   │   ├── ann-regression
│   │   ├── ann-clasification
│   │   ├── multi-layer-ann
│   │   ├── multi-output-ann
│   │   ├── random-forest
│   │   └── model-deptth-breadth
│   ├── meta-parameters
│   └── hyper-parameters
├── neural-network-concepts
│   ├── regularization
│   │   ├── prevent-overfitting-underfitting
│   │   ├── weight-reg
│   │   ├── dropout
│   │   ├── data-augmentation
│   │   ├── nomralization
│   │   │   ├── batch-nomralization
│   │   │   └── layer-nomralization
│   │   └── early-stopping
│   ├── optimization
│   │   ├── loss-cost-functions
│   │   ├── gradient-descent
│   │   ├── adaptive-optimization-algorithms
│   │   ├── learning-schedules
│   │   ├── weight-investigations
│   │   ├── numerical-stability
│   │   ├── meta-parameter optimization
│   │   └── hyper-parameter optimization
│   └── generalization
│   │   ├── cross-validation
│   │   ├── overfitting-underfitting
│   │   └── hyper-parameter-tuning
├── advanced-neural-network-architecture
│   ├── ffn
│   ├── cnn
│   │   ├── convolution
│   │   └── cnn-architecture
│   ├── rnn
│   │   ├── lstm
│   │   ├── gru
│   ├── gan
│   ├── gnn
│   ├── attention-mechanism
│   ├── transformer-models
│   │   └── bert
│   └── auto-encoders
└── model-training
```
