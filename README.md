# complete-deep-learning
Comprehensive Deep Learning concepts &amp; Architectures implemented using PyTorch.

## content-structure
```
complete-deep-learning
├── datasets
│   └── images-text-audio-misc
│
├── math-foundations
│   ├── linear-algebra
│   ├── calculus
│   └── probability-stats
│                                              
├── basic-neural-network-architecture
│   ├── neuron-perceptron
│   ├── neural-net-layers
│   │   ├── input-hidden-output-layers
│   ├── activation-functions
│   ├── ann (multilayer-perceptron)
│   │   ├── geometric-view
│   │   ├── ann-maths (forwardprop, error-los-cost, backrprop)
│   │   ├── ann-regression-clasification
│   │   ├── multi-layer-ann
│   │   ├── multi-output-ann
│   │   └── model-depth-breadth
│   ├── meta-parameters
│   └── hyper-parameters
│
├── neural-network-concepts
│   ├── regularization
│   │   ├── prevent-overfitting-underfitting
│   │   ├── weight-reg
│   │   ├── dropout
│   │   ├── data-augmentation
│   │   ├── nomralization
│   │   │   ├── batch-nomralization
│   │   │   └── layer-nomralization
│   │   └── early-stopping
│   ├── optimization
│   │   ├── loss-cost-functions
│   │   ├── gradient-descent
│   │   |   ├── vanilla-gd, sgd, minibatch-sgd
│   │   ├── adaptive-optimization-algorithms
│   │   |   ├── momentum, nag, adagrad, rmsprop, adam, adamw
│   │   ├── learning-schedules
│   │   ├── weight-investigations
│   │   ├── numerical-stability
│   │   ├── meta-parameter-optimization
│   │   └── hyper-parameter-optimization
│   └── generalization
│       ├── cross-validation
│       ├── overfitting-underfitting
│       └── hyper-parameter-tuning
│
├── computational-performance
│   └── run-on-gpu
│
├── advanced-neural-network-architecture
│   ├── ffn
│   ├── cnn-modern-cnn
│   │   ├── convolution
│   │   ├── cannonical-cnn
│   │   └── cnn-adv-architectures
│   ├── rnn
│   │   ├── lstm
│   │   ├── gru
│   ├── gan
│   ├── gnn
│   ├── attention-mechanism
│   ├── transformer-models
│   │   └── bert
│   └── encoders
│       └── autoencoders
│
├── model-training
│   ├── transfer-learning
│   ├── style-transfer
|   ├── training-loop-structure (epoch, batch, loss logging)
|   ├── callbacks (custom logging, checkpointing)
|   ├── experiment-tracking (Weights & Biases, TensorBoard)
│   └──  multitask-learning
│
└── model-evaluation
|   ├── accuracy-precision-recall-f1-auc-roc
|   └── confusion-matrix
│
└── papers-to-code
```
