<div style="display: flex; justify-content: space-around; align-items: center;">
  <img src="images/1.png" alt="Image 1" style="width: 49%; margin: 10px;">
  <img src="images/2.png" alt="Image 2" style="width: 49%; margin: 10px;">
<!--   <img src="images/3.JPG" alt="Image 3" style="width: 25%; margin: 10px;"> -->
<!--   <img src="images/4.JPG" alt="Image 2" style="width: 33%; margin: 10px;"> -->
</div>

## ğŸ“œ complete-deep-learning
#### ğŸ§  Overview 
Complete Deep Learning concepts &amp; Architectures implemented using PyTorch. This is a comprehensive Deep Learning roadmap and implementation using PyTorch â€” starting from core math foundations to state-of-the-art neural network architectures. The repository is designed to give a solid theoretical and practical understanding of deep learning, structured progressively to cover foundational concepts, mathematical intuition, model architectures, training, and evaluation.

#### ğŸ¯ Use Cases 
- Implementing DL algorithms/models/concepts using python & pytorch
- Learning & implementing the mathematical foundation of deep learning using python & pytorch
- Learn deep learning from scratch with a mathematical + implementation-first approach
- Study & build neural networks with PyTorch
- Study & build DL architectures with PyTorch
- Prepare for interviews and research
- Use as a practical teaching/learning guide
- Reference architecture and code for deep learning projects
  
#### ğŸŸ¢ Project Status
- Current Version: V1.0
- Actively maintained & expanded

#### ğŸ“‚ Repository Structure
```
complete-deep-learning
â”œâ”€â”€ images
â”‚
â”œâ”€â”€ datasets
â”‚   â””â”€â”€ images-text-audio-misc
â”‚
â”œâ”€â”€ math-foundations
â”‚   â”œâ”€â”€ linear-algebra
â”‚   â”œâ”€â”€ calculus
â”‚   â””â”€â”€ probability-stats
â”‚                                              
â”œâ”€â”€ basic-neural-network-architecture
â”‚   â”œâ”€â”€ neuron-perceptron
â”‚   â”œâ”€â”€ neural-net-layers
â”‚   â”‚   â”œâ”€â”€ input-hidden-output-layers
â”‚   â”œâ”€â”€ activation-functions
â”‚   â”œâ”€â”€ ann (multilayer-perceptron)
â”‚   â”‚   â”œâ”€â”€ geometric-view
â”‚   â”‚   â”œâ”€â”€ ann-maths (forwardprop, error-los-cost, backrprop)
â”‚   â”‚   â”œâ”€â”€ ann-regression-clasification
â”‚   â”‚   â”œâ”€â”€ multi-layer-ann
â”‚   â”‚   â”œâ”€â”€ multi-output-ann
â”‚   â”‚   â””â”€â”€ model-depth-breadth
â”‚   â”œâ”€â”€ meta-parameters
â”‚   â””â”€â”€ hyper-parameters
â”‚
â”œâ”€â”€ neural-network-concepts
â”‚   â”œâ”€â”€ regularization
â”‚   â”‚   â”œâ”€â”€ prevent-overfitting-underfitting
â”‚   â”‚   â”œâ”€â”€ weight-reg
â”‚   â”‚   â”œâ”€â”€ dropout
â”‚   â”‚   â”œâ”€â”€ data-augmentation
â”‚   â”‚   â”œâ”€â”€ nomralization
â”‚   â”‚   â”‚   â”œâ”€â”€ batch-nomralization
â”‚   â”‚   â”‚   â””â”€â”€ layer-nomralization
â”‚   â”‚   â””â”€â”€ early-stopping
â”‚   â”œâ”€â”€ optimization
â”‚   â”‚   â”œâ”€â”€ loss-cost-functions
â”‚   â”‚   â”œâ”€â”€ gradient-descent
â”‚   â”‚   |   â”œâ”€â”€ vanilla-gd, sgd, minibatch-sgd
â”‚   â”‚   â”œâ”€â”€ adaptive-optimization-algorithms
â”‚   â”‚   |   â”œâ”€â”€ momentum, nag, adagrad, rmsprop, adam, adamw
â”‚   â”‚   â”œâ”€â”€ learning-schedules
â”‚   â”‚   â”œâ”€â”€ weight-investigations
â”‚   â”‚   â”œâ”€â”€ numerical-stability
â”‚   â”‚   â”œâ”€â”€ meta-parameter-optimization
â”‚   â”‚   â””â”€â”€ hyper-parameter-optimization
â”‚   â””â”€â”€ generalization
â”‚       â”œâ”€â”€ cross-validation
â”‚       â”œâ”€â”€ overfitting-underfitting
â”‚       â””â”€â”€ hyper-parameter-tuning
â”‚
â”œâ”€â”€ computational-performance
â”‚   â””â”€â”€ run-on-gpu
â”‚
â”œâ”€â”€ advanced-neural-network-architecture
â”‚   â”œâ”€â”€ ffn
â”‚   â”œâ”€â”€ cnn-modern-cnn
â”‚   â”‚   â”œâ”€â”€ convolution
â”‚   â”‚   â”œâ”€â”€ cannonical-cnn
â”‚   â”‚   â””â”€â”€ cnn-adv-architectures
â”‚   â”œâ”€â”€ rnn
â”‚   â”‚   â”œâ”€â”€ lstm
â”‚   â”‚   â”œâ”€â”€ gru
â”‚   â”œâ”€â”€ gan
â”‚   â”œâ”€â”€ gnn
â”‚   â”œâ”€â”€ attention-mechanism
â”‚   â”œâ”€â”€ transformer-models
â”‚   â”‚   â””â”€â”€ bert
â”‚   â””â”€â”€ encoders
â”‚       â””â”€â”€ autoencoders
â”‚
â”œâ”€â”€ model-training
â”‚   â”œâ”€â”€ transfer-learning
â”‚   â”œâ”€â”€ style-transfer
|   â”œâ”€â”€ training-loop-structure (epoch, batch, loss logging)
|   â”œâ”€â”€ callbacks (custom logging, checkpointing)
|   â”œâ”€â”€ experiment-tracking (Weights & Biases, TensorBoard)
â”‚   â””â”€â”€  multitask-learning
â”‚
â””â”€â”€ model-evaluation
|   â”œâ”€â”€ accuracy-precision-recall-f1-auc-roc
|   â””â”€â”€ confusion-matrix
â”‚
â””â”€â”€ papers-to-code
```

### âœ¨ Features
- Covers Concepts, Mathematical implementations, DL nets and architectures
- Pure Python and Pytorch
- Modular, clean, and reusable code
- Educational and beginner-friendly
- Covers everything from perceptrons to transformers
- Clean, modular, and well-commented PyTorch implementations
- Visualization, training loops, and performance metrics
- Includes datasets for images, text, audio, and more
- Papers-to-Code section to implement SOTA research

<!-- ### ğŸ” Demo
<a href="https://youtu.be/Qor8kjsCJkA?si=7d1Mhc0KW4GQb3sF" target="_blank">
  <img src="https://img.youtube.com/vi/Qor8kjsCJkA/hqdefault.jpg" alt="YouTube Video" width="390" height="270">
</a> -->

### ğŸš€ Getting Started
- Knowledge Required : python, linear algebra, probability, statistics, numpy, matplotlib, scikit-learn, pytorch

<!-- ### ğŸ› ï¸ Hardware Requirements
- None
-->

#### ğŸ’» Software Requirements
- IDE (VS Code) or jupyter notebook or google colab
- Python 3
  
#### ğŸ›¡ï¸ Tech Stack
- Python , PyTorch, TorchVision ğŸ’»
- Numpy, Pandas, Matplotlib, Scikit-Learn ğŸ§©

<!--
### ğŸ–‡ï¸ Schematic
- none
-->

#### âš™ï¸ Installation
```
git clone https://github.com/pointer2Alvee/complete-deep-learning.git
cd comprehensive-deep-learning
```

#### ğŸ“– Usage
- Open .ipynb files inside each concept or NN architecture directory and
- Run them to see training/inference steps, plots, and results.

#### ğŸ” Contents Breakdown
##### ğŸ“š Math Foundations
- Linear Algebra, Calculus, Probability, Statistics

##### ğŸ§± Neural Network Basics
- Perceptrons, Layers, Activations, MLPs
- Forward & Backpropagation math from scratch
- Depth vs Breadth of models
- Regression & Classification using ANN

##### ğŸ”§ Deep Learning Concepts
- Regularization (Dropout, L2, Data Aug)
- Optimization (SGD, Adam, RMSProp, Schedules)
- Losses, Weight tuning, Meta & Hyperparams

##### âš™ï¸ Advanced Architectures
- CNNs (classic + modern)
- RNNs, LSTM, GRU
- GANs, GNNs
- Transformers & BERT
- Autoencoders

##### ğŸ‹ï¸â€â™‚ï¸ Model Training & Tracking
- Training Loops, Epochs, Batches
- Custom callbacks
- TensorBoard, Weights & Biases logging
- Transfer Learning & Style Transfer
- Multitask learning

##### ğŸ“Š Evaluation
- Accuracy, Precision, Recall, F1, AUC-ROC
- Confusion Matrix

##### ğŸ”¬ Research to Practice
- Paper Implementations â†’ PyTorch Code


### ğŸ§ª Sample Topics Implemented
- âœ… Forward & Backpropagation from scratch
- âœ… CNN with PyTorch
- âœ… Regularization (Dropout, Weight Decay)
- âœ… Adam vs SGD Performance Comparison
- âœ… Image Classification using Transfer Learning
- âœ… Transformer Attention Visualizations
- âœ… Autoencoder for Denoising
- âœ… Style Transfer with Pretrained CNN
  
- â³ Upcoming  : nlp, cv, llm, data engineering, feature engineering

### ğŸ§­ Roadmap
- [x] Build foundational math notebooks
- [ ] Implement perceptron â†’ MLP â†’ CNN
- [ ] Add reinforcement learning section
- [ ] Implement GAN, RNN, Transformer
- [ ] More research paper implementations

### ğŸ¤ Contributing
Contributions are welcomed!
1. Fork the repo. 
2. Create a branch: ```git checkout -b feature/YourFeature```
3. Commit changes: ```git commit -m 'Add some feature'```
4. Push to branch: ```git push origin feature/YourFeature```
5. Open a Pull Request.

### ğŸ“œLicense
Distributed under the MIT License. See LICENSE.txt for more information.

### ğŸ™Acknowledgements
- Special thanks to the open-source community / youtube for tools and resources.
